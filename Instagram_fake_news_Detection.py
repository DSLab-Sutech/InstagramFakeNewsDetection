# -*- coding: utf-8 -*-
"""Copy of linearRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J67o-Gj98fs1t56zSR15dhZpOfwDu5nk

# load data
"""

from google.colab import drive
drive.mount('/content/drive')

# read data
import pandas as pd
import sklearn.metrics as metrics
import matplotlib.pyplot as plt
data = pd.read_csv('/content/InBalanceAllData.csv') 
data.head(10)

# convert data to array structure
# seperate data and label 
import numpy as np
x = data [['avg_caption_lenght', 'avg_comment_length',	'has_url_in_bio',	'has_profile_picture',	'has_highlight_reels',	'post_count',
           'follower_count',	'following_count',	'engagement_page_rate',	'fake_followers_count',	'bio_length',	'avg_like_count',	'avg_comment_count',
           	'post_type',	'caption_length',	'hashtag_count',	'hashtag_rate',	'comment_count',	'like_count',	'view_count'] ]
y = data['label']
data = np.array(x)
label = np.array(y)
print("data shape: ",data.shape, '\nlabel shape: ', data.shape, '\n sample: \n', data[1] )

# normalize data 
from sklearn import preprocessing

def norm (norm_name , input_Data):
  out_norm_data = []
  if (norm_name == 'variance_scale'):
      out_norm_data = preprocessing.scale(input_Data,axis=0)
  elif (norm_name == 'min_max'):
      min_max_scaler = preprocessing.MinMaxScaler()
      out_norm_data = min_max_scaler.fit_transform(input_Data)

  return out_norm_data

norm_name = 'min_max'
data = norm(norm_name, data)
print( 'sample after normalization: \n\n', data[1])

xx =data [1500:-1]
yy =label [1500:-1]
data =  data[:1500]
label = label[:1500]

"""- example of data minimizing

     formula: 
         min_max:       x = (x - min(x)) / (max(x) - min(x))
         mean_variance  x = (x-mean) / variance
"""

X_train = np.array([[ 1., -1.,  2.],
                    [ 2.,  0.,  0.],
                    [ 0.,  1., -1.]])


min_max_scaler = preprocessing.MinMaxScaler( )
X_train_minmax = min_max_scaler.fit_transform(X_train)
X_train_meanvariance = preprocessing.scale(X_train, axis=0)
print("X_train_minmax: \n", X_train_minmax, "\nX_train_meanvariance: \n", X_train_meanvariance)

"""Next, we should split the training and testing data according to our dataset (0.8 and 0.2 in this case).

## svm
"""

# svm (suport vector machine)
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import sklearn.metrics as metrics
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score,  f1_score
from collections import Counter
import statistics
from mlxtend.evaluate import feature_importance_permutation


# from numpy.random import seed
# seed(3)

# kernel( poly, linear, rbf)

# K_fold = KFold(n_splits=10, shuffle=True, random_state=5,)
K_fold = StratifiedKFold(n_splits=7, shuffle=True, random_state=5,)

i = 1
res = {"accuracy":{"train":[], "test":[]}, 
       "precision":{"train":[], "test":[]}, 
       "recall":{"train":[], "test":[]}, 
       "F1":{"train":[], "test":[]}}

for train_idx, test_idx in K_fold.split(data, label) :
  print("\n  Fold: ", i)
  Train = data[train_idx]
  Train_label = label[train_idx]
  Test = data[test_idx]
  Test_label = label[test_idx]
  Test = np.append(Test, xx, axis=0)
  Test_label = np.append(Test_label, yy)
  # Test = xx
  # Test_label = yy
  print("Label count train:   ",Counter(Train_label),"\tLabel count test : ",Counter(Test_label))
  clf_o = make_pipeline(StandardScaler(), SVC(kernel='rbf',degree=5, gamma=0.000094, verbose=True, C=4))
  clf_o.fit(Train, Train_label)
  predict_train_label = clf_o.predict(Train)
  predict_test_label = clf_o.predict(Test)
  print('confution matrix:')
  print(metrics.confusion_matrix(predict_test_label, Test_label))
  print('Evaluation criteria:')

  # avg: macro, micro, weighted
  avg = 'macro'
  res['accuracy']['train'].append(accuracy_score(predict_train_label, Train_label))
  res['accuracy']['test'].append(accuracy_score(predict_test_label, Test_label))  
  res['precision']['train'].append(precision_score(predict_train_label, Train_label, average=avg))
  res['precision']['test'].append(precision_score(predict_test_label, Test_label, average=avg))
  res['recall']['train'].append(recall_score(predict_train_label, Train_label, average=avg))
  res['recall']['test'].append(recall_score(predict_test_label, Test_label, average=avg))
  res['F1']['train'].append(f1_score(predict_train_label, Train_label, average=avg))
  res['F1']['test'].append(f1_score(predict_test_label, Test_label, average=avg))

  print("\tAccuracy:\t train:   ",res['accuracy']['train'][i-1],  "\t test: ", res['accuracy']['test'][i-1],
        "\n\tPrecision:\t train:   ", res['precision']['train'][i-1],"\t test: ", res['precision']['test'][i-1],
        "\n\tRecall: \t train:   ", res['recall']['train'][i-1], "\t test: ", res['recall']['test'][i-1],
        "\n\tF1_measure: \t train:   ",  res['F1']['train'][i-1], "\t test: ", res['F1']['test'][i-1],
         )
  
  i+=1
  
  importance, imp_all = feature_importance_permutation(
    predict_method = clf_o.predict, 
    X=Test, y=Test_label,
    metric='r2', num_rounds=10, seed=1)
  # importance = clf_o.named_steps.svc.coef_[0]
  print("coeficient: \n", importance)
  plt.bar([x for x in range(len(importance))], importance)
  plt.xticks([x for x in range(len(importance))], x, rotation=90)
  plt.show()

  del clf_o

# calculate mean and variance for diffrent measures 

print('\n \033[94m Mean results: ')
print("\tAccuracy_mean:\t train:   ",statistics.mean(res['accuracy']['train']),  "\t test: ", statistics.mean(res['accuracy']['test']),
      "\n\tPrecision_mean:\t train:   ", statistics.mean(res['precision']['train']),"\t test: ", statistics.mean(res['precision']['test']),
      "\n\tRecall_mean: \t train:   ", statistics.mean(res['recall']['train']),"\t test: ", statistics.mean(res['recall']['test']),
      "\n\tF1_mean: \t train:   ",  statistics.mean(res['F1']['train']),"\t test: ", statistics.mean(res['F1']['test']),
       )
print(' \033[92m Varance results:')
print("\tAccuracy_variance:\t train:   ",statistics.variance(res['accuracy']['train']),  "\t test: ", statistics.variance(res['accuracy']['test']),
      "\n\tPrecision_variance:\t train:   ", statistics.variance(res['precision']['train']),"\t test: ", statistics.variance(res['precision']['test']),
      "\n\tRecall_variance: \t train:   ", statistics.variance(res['recall']['train']),"\t test: ", statistics.variance(res['recall']['test']),
      "\n\tF1_variance: \t\t train:   ",  statistics.variance(res['F1']['train']),"\t test: ", statistics.variance(res['F1']['test']),
       )

"""## logestic regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score,  f1_score
from collections import Counter
import statistics

from numpy.random import seed
seed(3)

# kernel( poly, linear, rbf)

# K_fold = KFold(n_splits=10, shuffle=True, random_state=5,)
K_fold = StratifiedKFold(n_splits=8, shuffle=True, random_state=5,)

i = 1
res = {"accuracy":{"train":[], "test":[]}, 
       "precision":{"train":[], "test":[]}, 
       "recall":{"train":[], "test":[]}, 
       "F1":{"train":[], "test":[]}}

for train_idx, test_idx in K_fold.split(data, label) :
  print("\n  Fold: ", i)
  Train = data[train_idx]
  Train_label = label[train_idx]
  Test = data[test_idx]
  Test_label = label[test_idx]
  Test = np.append(Test, xx, axis=0)
  Test_label = np.append(Test_label, yy)
  
  print("Label count train:   ",Counter(Train_label),"\tLabel count test : ",Counter(Test_label))
  
  clf_o = LogisticRegression(random_state=False, penalty='l2',solver='saga')
  clf_o.fit(Train, Train_label)
  predict_train_label = clf_o.predict(Train)
  predict_test_label = clf_o.predict(Test)
  print('confution matrix:')
  print(metrics.confusion_matrix(predict_test_label, Test_label))
  print('Evaluation criteria:')

  # avg: macro, micro, weighted
  avg = 'macro'
  res['accuracy']['train'].append(accuracy_score(predict_train_label, Train_label))
  res['accuracy']['test'].append(accuracy_score(predict_test_label, Test_label))  
  res['precision']['train'].append(precision_score(predict_train_label, Train_label, average=avg))
  res['precision']['test'].append(precision_score(predict_test_label, Test_label, average=avg))
  res['recall']['train'].append(recall_score(predict_train_label, Train_label, average=avg))
  res['recall']['test'].append(recall_score(predict_test_label, Test_label, average=avg))
  res['F1']['train'].append(f1_score(predict_train_label, Train_label, average=avg))
  res['F1']['test'].append(f1_score(predict_test_label, Test_label, average=avg))

  print("\tAccuracy:\t train:   ",res['accuracy']['train'][i-1],  "\t test: ", res['accuracy']['test'][i-1],
        "\n\tPrecision:\t train:   ", res['precision']['train'][i-1],"\t test: ", res['precision']['test'][i-1],
        "\n\tRecall: \t train:   ", res['recall']['train'][i-1], "\t test: ", res['recall']['test'][i-1],
        "\n\tF1_measure: \t train:   ",  res['F1']['train'][i-1], "\t test: ", res['F1']['test'][i-1],
         )
  
  i+=1

  importance = clf_o.coef_[0]
  print("importance: \n", importance)
  plt.bar([x for x in range(len(importance))], importance)
  plt.xticks([x for x in range(len(importance))], x, rotation=90)
  plt.show()

  del clf_o

# calculate mean and variance for diffrent measures 

print('\n \033[94m Mean results: ')
print("\tAccuracy_mean:\t train:   ",statistics.mean(res['accuracy']['train']),  "\t test: ", statistics.mean(res['accuracy']['test']),
      "\n\tPrecision_mean:\t train:   ", statistics.mean(res['precision']['train']),"\t test: ", statistics.mean(res['precision']['test']),
      "\n\tRecall_mean: \t train:   ", statistics.mean(res['recall']['train']),"\t test: ", statistics.mean(res['recall']['test']),
      "\n\tF1_mean: \t train:   ",  statistics.mean(res['F1']['train']),"\t test: ", statistics.mean(res['F1']['test']),
       )
print(' \033[92m Varance results:')
print("\tAccuracy_variance:\t train:   ",statistics.variance(res['accuracy']['train']),  "\t test: ", statistics.variance(res['accuracy']['test']),
      "\n\tPrecision_variance:\t train:   ", statistics.variance(res['precision']['train']),"\t test: ", statistics.variance(res['precision']['test']),
      "\n\tRecall_variance: \t train:   ", statistics.variance(res['recall']['train']),"\t test: ", statistics.variance(res['recall']['test']),
      "\n\tF1_variance: \t\t train:   ",  statistics.variance(res['F1']['train']),"\t test: ", statistics.variance(res['F1']['test']),
       )

"""## naive bayes"""

from sklearn.naive_bayes import GaussianNB, BernoulliNB
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score,  f1_score
from collections import Counter
import statistics
import sklearn.metrics as metrics

from numpy.random import seed
seed(3)

# kernel( poly, linear, rbf)

# K_fold = KFold(n_splits=10, shuffle=True, random_state=5,)
K_fold = StratifiedKFold(n_splits=7, shuffle=True, random_state=5,)

i = 1
res = {"accuracy":{"train":[], "test":[]}, 
       "precision":{"train":[], "test":[]}, 
       "recall":{"train":[], "test":[]}, 
       "F1":{"train":[], "test":[]}}

for train_idx, test_idx in K_fold.split(data, label) :
  print("\n  Fold: ", i)
  Train = data[train_idx]
  Train_label = label[train_idx]
  Test = data[test_idx]
  Test_label = label[test_idx]
  Test = np.append(Test, xx, axis=0)
  Test_label = np.append(Test_label, yy)
  print("Label count train:   ",Counter(Train_label),"\tLabel count test : ",Counter(Test_label))
  clf_o = BernoulliNB(alpha=0.01, fit_prior=True,binarize=0.55)
  clf_o.fit(Train, Train_label)
  predict_train_label = clf_o.predict(Train)
  predict_test_label = clf_o.predict(Test)
  print('confution matrix:')
  print(metrics.confusion_matrix(predict_test_label, Test_label))
  print('Evaluation criteria:')

  # avg: macro, micro, weighted
  avg = 'macro'
  res['accuracy']['train'].append(accuracy_score(predict_train_label, Train_label))
  res['accuracy']['test'].append(accuracy_score(predict_test_label, Test_label))  
  res['precision']['train'].append(precision_score(predict_train_label, Train_label, average=avg))
  res['precision']['test'].append(precision_score(predict_test_label, Test_label, average=avg))
  res['recall']['train'].append(recall_score(predict_train_label, Train_label, average=avg))
  res['recall']['test'].append(recall_score(predict_test_label, Test_label, average=avg))
  res['F1']['train'].append(f1_score(predict_train_label, Train_label, average=avg))
  res['F1']['test'].append(f1_score(predict_test_label, Test_label, average=avg))

  print("\tAccuracy:\t train:   ",res['accuracy']['train'][i-1],  "\t test: ", res['accuracy']['test'][i-1],
        "\n\tPrecision:\t train:   ", res['precision']['train'][i-1],"\t test: ", res['precision']['test'][i-1],
        "\n\tRecall: \t train:   ", res['recall']['train'][i-1], "\t test: ", res['recall']['test'][i-1],
        "\n\tF1_measure: \t train:   ",  res['F1']['train'][i-1], "\t test: ", res['F1']['test'][i-1],
         )

  i+=1

  im, imp_all = feature_importance_permutation(
    predict_method = clf_o.predict, 
    X=data, y=label,
    metric='accuracy', num_rounds=10, seed=1)
  print("importance: \n", im)
  plt.bar([x for x in range(len(im))], im)
  plt.xticks([x for x in range(len(im))], x, rotation=90)
  plt.show()

  del clf_o

# calculate mean and variance for diffrent measures 

print('\n \033[94m Mean results: ')
print("\tAccuracy_mean:\t train:   ",statistics.mean(res['accuracy']['train']),  "\t test: ", statistics.mean(res['accuracy']['test']),
      "\n\tPrecision_mean:\t train:   ", statistics.mean(res['precision']['train']),"\t test: ", statistics.mean(res['precision']['test']),
      "\n\tRecall_mean: \t train:   ", statistics.mean(res['recall']['train']),"\t test: ", statistics.mean(res['recall']['test']),
      "\n\tF1_mean: \t train:   ",  statistics.mean(res['F1']['train']),"\t test: ", statistics.mean(res['F1']['test']),
       )
print(' \033[92m Varance results:')
print("\tAccuracy_variance:\t train:   ",statistics.variance(res['accuracy']['train']),  "\t test: ", statistics.variance(res['accuracy']['test']),
      "\n\tPrecision_variance:\t train:   ", statistics.variance(res['precision']['train']),"\t test: ", statistics.variance(res['precision']['test']),
      "\n\tRecall_variance: \t train:   ", statistics.variance(res['recall']['train']),"\t test: ", statistics.variance(res['recall']['test']),
      "\n\tF1_variance: \t\t train:   ",  statistics.variance(res['F1']['train']),"\t test: ", statistics.variance(res['F1']['test']),
       )

"""## random forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score,  f1_score
from collections import Counter
import statistics
import sklearn.metrics as metrics

from numpy.random import seed
seed(3)

# kernel( poly, linear, rbf)

K_fold = KFold(n_splits=10, shuffle=True, random_state=5,)
# K_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=5,)

i = 1
res = {"accuracy":{"train":[], "test":[]}, 
       "precision":{"train":[], "test":[]}, 
       "recall":{"train":[], "test":[]}, 
       "F1":{"train":[], "test":[]}}

for train_idx, test_idx in K_fold.split(data, label) :
  print("\n  Fold: ", i)
  Train = data[train_idx]
  Train_label = label[train_idx]
  Test = data[test_idx]
  Test_label = label[test_idx]
  Test = np.append(Test, xx, axis=0)
  Test_label = np.append(Test_label, yy)
  print("Label count train:   ",Counter(Train_label),"\tLabel count test : ",Counter(Test_label))
  clf_o = RandomForestClassifier()
  clf_o.fit(Train, Train_label)
  predict_train_label = clf_o.predict(Train)
  predict_test_label = clf_o.predict(Test)
  print('confution matrix:')
  print(metrics.confusion_matrix(predict_test_label, Test_label))
  print('Evaluation criteria:')

  # avg: macro, micro, weighted
  avg = 'macro'
  res['accuracy']['train'].append(accuracy_score(predict_train_label, Train_label))
  res['accuracy']['test'].append(accuracy_score(predict_test_label, Test_label))  
  res['precision']['train'].append(precision_score(predict_train_label, Train_label, average=avg))
  res['precision']['test'].append(precision_score(predict_test_label, Test_label, average=avg))
  res['recall']['train'].append(recall_score(predict_train_label, Train_label, average=avg))
  res['recall']['test'].append(recall_score(predict_test_label, Test_label, average=avg))
  res['F1']['train'].append(f1_score(predict_train_label, Train_label, average=avg))
  res['F1']['test'].append(f1_score(predict_test_label, Test_label, average=avg))

  print("\tAccuracy:\t train:   ",res['accuracy']['train'][i-1],  "\t test: ", res['accuracy']['test'][i-1],
        "\n\tPrecision:\t train:   ", res['precision']['train'][i-1],"\t test: ", res['precision']['test'][i-1],
        "\n\tRecall: \t train:   ", res['recall']['train'][i-1], "\t test: ", res['recall']['test'][i-1],
        "\n\tF1_measure: \t train:   ",  res['F1']['train'][i-1], "\t test: ", res['F1']['test'][i-1],
         )
  
  i+=1
  print('feature_ importance: \n', clf_o.feature_importances_)
  importance = clf_o.feature_importances_

  plt.bar([x for x in range(len(importance))], importance)
  plt.xticks([x for x in range(len(importance))], x, rotation=90)
  plt.show()

# calculate mean and variance for diffrent measures 

print('\n \033[94m Mean results: ')
print("\tAccuracy_mean:\t train:   ",statistics.mean(res['accuracy']['train']),  "\t test: ", statistics.mean(res['accuracy']['test']),
      "\n\tPrecision_mean:\t train:   ", statistics.mean(res['precision']['train']),"\t test: ", statistics.mean(res['precision']['test']),
      "\n\tRecall_mean: \t train:   ", statistics.mean(res['recall']['train']),"\t test: ", statistics.mean(res['recall']['test']),
      "\n\tF1_mean: \t train:   ",  statistics.mean(res['F1']['train']),"\t test: ", statistics.mean(res['F1']['test']),
       )
print(' \033[92m Varance results:')
print("\tAccuracy_variance:\t train:   ",statistics.variance(res['accuracy']['train']),  "\t test: ", statistics.variance(res['accuracy']['test']),
      "\n\tPrecision_variance:\t train:   ", statistics.variance(res['precision']['train']),"\t test: ", statistics.variance(res['precision']['test']),
      "\n\tRecall_variance: \t train:   ", statistics.variance(res['recall']['train']),"\t test: ", statistics.variance(res['recall']['test']),
      "\n\tF1_variance: \t\t train:   ",  statistics.variance(res['F1']['train']),"\t test: ", statistics.variance(res['F1']['test']),
       )

"""## knn"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score,  f1_score
from collections import Counter
import statistics
import sklearn.metrics as metrics
from sklearn.inspection import permutation_importance

from numpy.random import seed
seed(3)

# kernel( poly, linear, rbf)

K_fold = KFold(n_splits=7, shuffle=True, random_state=5,)
# K_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=5,)

i = 1
res = {"accuracy":{"train":[], "test":[]}, 
       "precision":{"train":[], "test":[]}, 
       "recall":{"train":[], "test":[]}, 
       "F1":{"train":[], "test":[]}}

for train_idx, test_idx in K_fold.split(data, label) :
  print("\n  Fold: ", i)
  Train = data[train_idx]
  Train_label = label[train_idx]
  Test = data[test_idx]
  Test_label = label[test_idx]
  Test = np.append(Test, xx, axis=0)
  Test_label = np.append(Test_label, yy)
  print("Label count train:   ",Counter(Train_label),"\tLabel count test : ",Counter(Test_label))
  clf_o = KNeighborsClassifier(n_neighbors=15,)
  clf_o.fit(Train, Train_label)
  predict_train_label = clf_o.predict(Train)
  predict_test_label = clf_o.predict(Test)
  print('confution matrix:')
  print(metrics.confusion_matrix(predict_test_label, Test_label))
  print('Evaluation criteria:')

  # avg: macro, micro, weighted
  avg = 'macro'
  res['accuracy']['train'].append(accuracy_score(predict_train_label, Train_label))
  res['accuracy']['test'].append(accuracy_score(predict_test_label, Test_label))  
  res['precision']['train'].append(precision_score(predict_train_label, Train_label, average=avg))
  res['precision']['test'].append(precision_score(predict_test_label, Test_label, average=avg))
  res['recall']['train'].append(recall_score(predict_train_label, Train_label, average=avg))
  res['recall']['test'].append(recall_score(predict_test_label, Test_label, average=avg))
  res['F1']['train'].append(f1_score(predict_train_label, Train_label, average=avg))
  res['F1']['test'].append(f1_score(predict_test_label, Test_label, average=avg))

  print("\tAccuracy:\t train:   ",res['accuracy']['train'][i-1],  "\t test: ", res['accuracy']['test'][i-1],
        "\n\tPrecision:\t train:   ", res['precision']['train'][i-1],"\t test: ", res['precision']['test'][i-1],
        "\n\tRecall: \t train:   ", res['recall']['train'][i-1], "\t test: ", res['recall']['test'][i-1],
        "\n\tF1_measure: \t train:   ",  res['F1']['train'][i-1], "\t test: ", res['F1']['test'][i-1],
         )
  
  i+=1
  
  results = permutation_importance(clf_o, Train, Train_label, scoring='neg_mean_squared_error')
  # get importance
  importance = results.importances_mean
  print("coeficient: \n", importance)
  
  plt.bar([x for x in range(len(importance))], importance)
  plt.xticks([x for x in range(len(importance))], x, rotation=90)
  plt.show()

  
  del clf_o

# calculate mean and variance for diffrent measures 

print('\n \033[94m Mean results: ')
print("\tAccuracy_mean:\t train:   ",statistics.mean(res['accuracy']['train']),  "\t test: ", statistics.mean(res['accuracy']['test']),
      "\n\tPrecision_mean:\t train:   ", statistics.mean(res['precision']['train']),"\t test: ", statistics.mean(res['precision']['test']),
      "\n\tRecall_mean: \t train:   ", statistics.mean(res['recall']['train']),"\t test: ", statistics.mean(res['recall']['test']),
      "\n\tF1_mean: \t train:   ",  statistics.mean(res['F1']['train']),"\t test: ", statistics.mean(res['F1']['test']),
       )
print(' \033[92m Varance results:')
print("\tAccuracy_variance:\t train:   ",statistics.variance(res['accuracy']['train']),  "\t test: ", statistics.variance(res['accuracy']['test']),
      "\n\tPrecision_variance:\t train:   ", statistics.variance(res['precision']['train']),"\t test: ", statistics.variance(res['precision']['test']),
      "\n\tRecall_variance: \t train:   ", statistics.variance(res['recall']['train']),"\t test: ", statistics.variance(res['recall']['test']),
      "\n\tF1_variance: \t\t train:   ",  statistics.variance(res['F1']['train']),"\t test: ", statistics.variance(res['F1']['test']),
       )